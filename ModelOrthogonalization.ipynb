{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Space Orthogonalization for Predictive Bayesian Model Combination\n",
    "\n",
    "The following notebook outlines an implementation of the model orthogonalization and Bayesian model combination procedure detailed in \\<Waiting for link :D\\>\n",
    "\n",
    "The layout is as follows:\n",
    "\n",
    "- [Preamble](#preamble)\n",
    "- [Setting up the Models](#setting-up-the-models)\n",
    "    - [Model Parameters and Dataset Variables](#model-parameters-and-data-variables)\n",
    "    - [Dataset Preparation](#dataset-preparation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "Up first we have a ton of preamble content that is likely only interesting to a select few, you can safely skip this part if you're just curious about the model orthogonalization and model combination aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import corner \n",
    "\n",
    "\n",
    "\n",
    "#Some colors that Pablo likes:\n",
    "colors = [\n",
    "    \"#ff7f0e\",\n",
    "\n",
    "    \"#1f77b4\",\n",
    "\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "] \n",
    "\n",
    "markers = [\n",
    "    \"o\",  # Circle\n",
    "    \"^\",  # Triangle up\n",
    "    \"s\",  # Square\n",
    "    \"P\",  # Plus (filled)\n",
    "    \"*\",  # Star\n",
    "    \"X\",  # X (filled)\n",
    "    \"D\",  # Diamond\n",
    "    \"H\",  # Hexagon\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up some helper functions for plots that will come later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bars(values, labels, title=\"Bar Plot\", color='blue'):\n",
    "    \"\"\"\n",
    "    Create a bar plot based on the provided values and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - values (list): A list of numerical values for the bars.\n",
    "    - labels (list): A list of labels for each bar.\n",
    "    - title (str): The title for the bar plot. Default is \"Bar Plot\".\n",
    "    - color (str): Color for the bars. Default is 'blue'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(values) != len(labels):\n",
    "        raise ValueError(\"Length of values and labels should be the same.\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, values, color=color)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Vt Coordinates')\n",
    "    plt.xticks(labels,fontsize=10,rotation='vertical')\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def Plotter2D_single(data):\n",
    "    xvals,yvals,zvals = data\n",
    "    #A plotter to see principal components in 2D\n",
    "    plt.rc(\"xtick\", labelsize=25)\n",
    "    plt.rc(\"ytick\", labelsize=25)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8), dpi=200)\n",
    "\n",
    "    # Create scatter plot\n",
    "    sc = ax.scatter(xvals, yvals, c=zvals, s=25, cmap='plasma',marker='s')\n",
    "    # plt.colorbar(sc, label='Z-Value')\n",
    "    cbar = plt.colorbar(sc, ax=ax)\n",
    "    cbar.set_label('Z-Value', fontsize=25) \n",
    "    cbar.ax.tick_params(labelsize=20) \n",
    "    plt.xlabel('Neutrons',fontsize=25)\n",
    "    plt.ylabel('Protons',fontsize=25)\n",
    "\n",
    "    ax.grid(True)\n",
    "    # ax.axis('equal')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def Plotter3D_single(data,elev=30,azim=-60):\n",
    "    xvals,yvals,zvals = data\n",
    "    #A plotter to see principal components in 3D\n",
    "    plt.rc(\"xtick\", labelsize=15)\n",
    "    plt.rc(\"ytick\", labelsize=15)\n",
    "\n",
    "    z_min = zvals.min()\n",
    "    z_max = zvals.max()\n",
    "    z_normalized = (zvals - z_min) / (z_max - z_min)\n",
    "\n",
    "    # Create figure for 3D plot\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=200)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "    # Define the size of the bars\n",
    "    dx = dy = 1.5  # Width of the bars in the x and y direction\n",
    "    dz = z_normalized        # Height of the bars (z values)\n",
    "\n",
    "    # Create 3D bar plot\n",
    "    ax.bar3d(xvals, yvals, np.zeros_like(zvals), dx, dy, dz, color=plt.cm.plasma(z_normalized))\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim) \n",
    "\n",
    "    # Setting labels and title\n",
    "    ax.set_xlabel('Neutrons')\n",
    "    ax.set_ylabel('Protons')\n",
    "    ax.set_zlabel('Scaled Z')\n",
    "\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "def Plotter2D(ax,xvals,yvals,zvals):\n",
    "    #A plotter to see principal components in 2D, part of the big plotter\n",
    "\n",
    "\n",
    "\n",
    "    # Create scatter plot\n",
    "    sc = ax.scatter(xvals, yvals, c=zvals, s=25, cmap='plasma',marker='s')\n",
    "    # plt.colorbar(sc, label='Z-Value')\n",
    "    cbar = plt.colorbar(sc, ax=ax)\n",
    "    cbar.set_label('Z-Value', fontsize=25) \n",
    "    cbar.ax.tick_params(labelsize=20) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.xlabel('Neutrons',fontsize=25)\n",
    "    plt.ylabel('Protons',fontsize=25)\n",
    "\n",
    "    ax.grid(True)\n",
    "    ax.axis('equal')\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "def Plotter3D(ax,xvals,yvals,zvals,elev=30,azim=-60):\n",
    "    #A plotter to see principal components in 3D, part of the big plotter\n",
    "\n",
    "\n",
    "    z_min = zvals.min()\n",
    "    z_max = zvals.max()\n",
    "    z_normalized = (zvals - z_min) / (z_max - z_min)\n",
    "\n",
    "    # Create figure for 3D plot\n",
    "    # fig = plt.figure(figsize=(8, 6), dpi=200)\n",
    "    # ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "    # Define the size of the bars\n",
    "    dx = dy = 1.5  # Width of the bars in the x and y direction\n",
    "    dz = z_normalized        # Height of the bars (z values)\n",
    "\n",
    "    # Create 3D bar plot\n",
    "    ax.bar3d(xvals, yvals, np.zeros_like(zvals), dx, dy, dz, color=plt.cm.plasma(z_normalized))\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim) \n",
    "\n",
    "    # Setting labels and title\n",
    "    ax.set_xlabel('Neutrons')\n",
    "    ax.set_ylabel('Protons')\n",
    "    ax.set_zlabel('Scaled Z')\n",
    "\n",
    "\n",
    "    # plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "def PlotMultiple(data_sets,angles_sheet=None):\n",
    "    # Determine the number of rows needed for the plots\n",
    "    n_rows = len(data_sets)\n",
    "    # Each data_sets element should look like [x,y,z]\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 8 * n_rows), dpi=200)\n",
    "\n",
    "    for i, (xvals, yvals, zvals) in enumerate(data_sets):\n",
    "        # 2D plot\n",
    "        ax_2d = fig.add_subplot(n_rows, 2, 2 * i + 1)\n",
    "        Plotter2D(ax_2d, xvals, yvals, zvals)\n",
    "\n",
    "        # 3D plot\n",
    "        ax_3d = fig.add_subplot(n_rows, 2, 2 * i + 2, projection='3d')\n",
    "        if angles_sheet == None:\n",
    "            Plotter3D(ax_3d, xvals, yvals, zvals)\n",
    "        else:\n",
    "            Plotter3D(ax_3d, xvals, yvals, zvals,elev=angles_sheet[0],azim=angles_sheet[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up are some helpful functions for splitting up datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_points_random(list1,random_chance):\n",
    "    \"\"\"\n",
    "    Separates points in list1 into two groups randomly\n",
    "\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    train_list_coordinates=[]\n",
    "    test_list_coordinates=[]\n",
    "\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        point1=list1[i]\n",
    "        val=np.random.rand()\n",
    "        if val<=random_chance:\n",
    "            train.append(point1)\n",
    "            train_list_coordinates.append(i)\n",
    "        else:\n",
    "            test.append(point1)\n",
    "            test_list_coordinates.append(i)\n",
    "\n",
    "    return np.array(train), np.array(test), np.array(train_list_coordinates), np.array(test_list_coordinates)\n",
    "\n",
    "\n",
    "def separate_points_distance(list1, list2, distance):\n",
    "    \"\"\"\n",
    "    Separates points in list1 into two groups based on their proximity to any point in list2.\n",
    "\n",
    "    :param list1: List of (x, y) tuples.\n",
    "    :param list2: List of (x, y) tuples.\n",
    "    :param distance: The threshold distance to determine proximity.\n",
    "    :return: Two lists - close_points and distant_points.\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    train_list_coordinates=[]\n",
    "    test_list_coordinates=[]\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        point1=list1[i]\n",
    "        close = False\n",
    "        for point2 in list2:\n",
    "            if np.linalg.norm(np.array(point1) - np.array(point2)) <= distance:\n",
    "                close = True\n",
    "                break\n",
    "        if close:\n",
    "            train.append(point1)\n",
    "            train_list_coordinates.append(i)\n",
    "        else:\n",
    "            test.append(point1)\n",
    "            test_list_coordinates.append(i)\n",
    "\n",
    "    return np.array(train), np.array(test), np.array(train_list_coordinates), np.array(test_list_coordinates)\n",
    "\n",
    "def separate_points_distance_allSets(list1, list2, distance1, distance2):\n",
    "    \"\"\"\n",
    "    Separates points in list1 into three groups based on their proximity to any point in list2.\n",
    "\n",
    "    :param list1: List of (x, y) tuples.\n",
    "    :param list2: List of (x, y) tuples.\n",
    "    :param distance: The threshold distance to determine proximity.\n",
    "    :return: Two lists - close_points and distant_points.\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    validation=[]\n",
    "    test = []\n",
    "\n",
    "    train_list_coordinates=[]\n",
    "    validation_list_coordinates=[]\n",
    "    test_list_coordinates=[]\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        point1=list1[i]\n",
    "        close = False\n",
    "        for point2 in list2:\n",
    "            if np.linalg.norm(np.array(point1) - np.array(point2)) <= distance1:\n",
    "                close = True\n",
    "                break\n",
    "        if close:\n",
    "            train.append(point1)\n",
    "            train_list_coordinates.append(i)\n",
    "        else:\n",
    "            close2=False\n",
    "            for point2 in list2:\n",
    "                if np.linalg.norm(np.array(point1) - np.array(point2)) <= distance2:\n",
    "                    close2 = True\n",
    "                    break\n",
    "            if close2==True:\n",
    "                validation.append(point1)\n",
    "                validation_list_coordinates.append(i)\n",
    "            else:\n",
    "                test.append(point1)\n",
    "                test_list_coordinates.append(i)                \n",
    "\n",
    "    return np.array(train),np.array(validation), np.array(test), np.array(train_list_coordinates),  np.array(validation_list_coordinates),np.array(test_list_coordinates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Models \n",
    "\n",
    "For this example application we are considering a simple nuclear model to generate the data so that we can have a firm control over truth that we will lack in the real case. In this case we are using an extended liquid drop model and variations on those models to construct a set of models ranging in 'badness'. These model definitions and model parameters are worth playing around with to check how the whole machinery works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDM_extended(params, x): \n",
    "    #x = (n,z)\n",
    "    #params= parameters (volume, surface, curv, sym, ssym, sym_2, Coulomb)\n",
    "    \n",
    "    n=x[0]\n",
    "    z=x[1]\n",
    "    A = n + z\n",
    "    I = (n-z)/(n+z)\n",
    "\n",
    "\n",
    "    return A*params[0] + params[1] * A ** (2/3) + params[2] * A ** (1/3)  +  params[3] * I ** 2*A + \\\n",
    "                + params[4] * (I ** 2) * A ** (2/3) + params[5] * (I ** 4)*A + params[6]*((z**2)/((A)**(1/3))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters and Data Variables\n",
    "\n",
    "The parameters themselves are changed in the following cells, along with several features controlling the noise added to the datasets and dataset splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global variables####\n",
    "\n",
    "# SkO values selected for the truth\n",
    "\n",
    "truth_params = [-15.835, 17.3, 9, 31.98, -58, - 4.5 * (163.5 ** 2) * (0.1605**2) / 223.5,0.57]\n",
    "\n",
    "PerfectM_params = [-15.835, 17.3, 9, 31.98, -58, - 4.5 * (163.5 ** 2) * (0.1605**2) / 223.5,0.57]\n",
    "\n",
    "# SLy4 \n",
    "GoodM_params = [-15.972, 18.4, 9, 32.01, -54, - 4.5 * (95.97 ** 2) * (0.1596**2) / 230.1, 0.57]\n",
    "# NL_1\n",
    "BadM_params = [-16.425, 18.8, 9, 43.48, -110, -4.5 *(311.18 **2) *(0.1518**2)/211.3, 0.57]\n",
    "\n",
    "# NL_1 \n",
    "TerribleM_params = [-15.972, 18.4, 9, 0, 0, 0, 0.57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noises\n",
    "##########################\n",
    "\n",
    "#Turn off term (to make colinear examples). Turn it to 1 or 0\n",
    "\n",
    "switch=1\n",
    "\n",
    "#Noise terms to be added to the parameters\n",
    "params_noise_term_p=0.000*switch\n",
    "params_noise_term_g=0.002*switch\n",
    "params_noise_term_b=0.002*switch\n",
    "params_noise_term_t=0.002*switch\n",
    "\n",
    "\n",
    "overall_output_noise=1*switch   # Noise added to each observation coming from models\n",
    "\n",
    "# overall_output_noise=0.000   # Noise added to each observation coming from models\n",
    "\n",
    "overall_data_noise= 1*switch  #Noise added to the \"true\" generated data itself\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Data separation (training, testing, validation) set ups\n",
    "##########################\n",
    "\n",
    "#Number used for when dividing training and testing randomly uniformly (interpolating)\n",
    "TestingFraction=0.34\n",
    "\n",
    "#Number used for when dividing training and testing by how close they are to the stable nuclei\n",
    "distance=2\n",
    "\n",
    "#Coordinates to truncate so the LDM evaluations are not done below these isotopes\n",
    "minimumZ=8\n",
    "minimumN=8\n",
    "\n",
    "#Even-even nuclei only? Set this flag to True\n",
    "even_even=True\n",
    "\n",
    "#Centering set up\n",
    "##########################\n",
    "\n",
    "centering_data=True\n",
    "\n",
    "\n",
    "#Scenario set up\n",
    "##########################\n",
    "\n",
    "n_perfect =0\n",
    "n_good = 3\n",
    "n_bad = 5\n",
    "n_terrible=10\n",
    "\n",
    "n_classes=[n_perfect,n_good,n_bad,n_terrible]\n",
    "\n",
    "#Re-labeling terrible by \"bad\" so we are not too insensitive to the poor model\n",
    "n_Labels=[\"Perfect\", \"Good\", \"Inter.\",\"Bad\"]\n",
    "\n",
    "\n",
    "\n",
    "#Isotope chain for 1-D plotting selection\n",
    "##########################\n",
    "\n",
    "Selected_element=50\n",
    "Selected_element_name=\"Sn\"\n",
    "\n",
    "\n",
    "\n",
    "#Computing constrained MCMC? (takes ~ + 5 minutes extra time)\n",
    "##########################\n",
    "\n",
    "computing_MCMC=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "Now we should chop up our dataset and select our training, testing, and validation sets. We will use the stable nuclei as a starting point and then select everything else based around those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_coordinates_full=np.loadtxt(\"Stable-Isotopes.txt\")\n",
    "\n",
    "stable_coordinates=[]\n",
    "\n",
    "if(even_even):\n",
    "    for i in range(len(stable_coordinates_full)):\n",
    "        if (stable_coordinates_full[i][0]>=minimumN) & (stable_coordinates_full[i][1]>=minimumZ) & (stable_coordinates_full[i][0]% 2 == 0) & (stable_coordinates_full[i][1]% 2 == 0) :\n",
    "            stable_coordinates.append(stable_coordinates_full[i])\n",
    "else:\n",
    "    for i in range(len(stable_coordinates_full)):\n",
    "        if (stable_coordinates_full[i][0]>=minimumN) & (stable_coordinates_full[i][1]>=minimumZ) :\n",
    "            stable_coordinates.append(stable_coordinates_full[i])\n",
    "\n",
    "stable_coordinates=np.array(stable_coordinates)\n",
    "\n",
    "\n",
    "Full_set_2020=np.loadtxt(\"AME2020.txt\")\n",
    "AME2020_even=[]\n",
    "\n",
    "if(even_even):\n",
    "    for i in range(len(Full_set_2020)):\n",
    "        if (Full_set_2020[i][0]>=minimumN) & (Full_set_2020[i][1]>=minimumZ) & (Full_set_2020[i][0]% 2 == 0) & (Full_set_2020[i][1]% 2 == 0) :\n",
    "            AME2020_even.append([Full_set_2020[i][0],Full_set_2020[i][1]])\n",
    "else:\n",
    "    for i in range(len(Full_set_2020)):\n",
    "        if (Full_set_2020[i][0]>=minimumN) & (Full_set_2020[i][1]>=minimumZ) :\n",
    "            AME2020_even.append([Full_set_2020[i][0],Full_set_2020[i][1]])\n",
    "\n",
    "Full_set=np.array(AME2020_even)\n",
    "\n",
    "distance1=2\n",
    "distance2=3\n",
    "\n",
    "training_set, validation_set, test_set,train_coordinates0, validation_coordinates0,test_coordinates0=separate_points_distance_allSets(Full_set, stable_coordinates, distance1,distance2)\n",
    "\n",
    "train_coordinates=[]\n",
    "for i in range(len(Full_set)):\n",
    "    isotope2020=Full_set[i]\n",
    "    for j in range(len(training_set)):\n",
    "       train_isotope=training_set[j]\n",
    "       if (isotope2020[0]==train_isotope[0]) & (isotope2020[1]==train_isotope[1]):\n",
    "           train_coordinates.append(i)\n",
    "           break\n",
    "train_coordinates=np.array(train_coordinates)\n",
    "\n",
    "validation_coordinates=[]\n",
    "for i in range(len(Full_set)):\n",
    "    isotope2020=Full_set[i]\n",
    "    for j in range(len(validation_set)):\n",
    "       validation_isotope=validation_set[j]\n",
    "       if (isotope2020[0]==validation_isotope[0]) & (isotope2020[1]==validation_isotope[1]):\n",
    "           validation_coordinates.append(i)\n",
    "           break\n",
    "validation_coordinates=np.array(validation_coordinates)\n",
    "\n",
    "test_coordinates=[]\n",
    "for i in range(len(Full_set)):\n",
    "    isotope2020=Full_set[i]\n",
    "    for j in range(len(test_set)):\n",
    "       test_isotope=test_set[j]\n",
    "       if (isotope2020[0]==test_isotope[0]) & (isotope2020[1]==test_isotope[1]):\n",
    "           test_coordinates.append(i)\n",
    "           break\n",
    "test_coordinates=np.array(test_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data landscape\n",
    "\n",
    "It can be instructive to visual your data split, especially if you're trying to construct a super model that is tuned for extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"xtick\", labelsize=35)\n",
    "plt.rc(\"ytick\", labelsize=35)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10), dpi=100)\n",
    "\n",
    "\n",
    "color_trainig=colors[9]\n",
    "color_validation='orange'\n",
    "color_testing=colors[3]\n",
    "\n",
    "marker_trainig='s'\n",
    "marker_validation='*'\n",
    "marker_testing='o'\n",
    "\n",
    "\n",
    "\n",
    "size_trainig=30\n",
    "size_validation=80\n",
    "size_testing=35\n",
    "\n",
    "alpha_trainig=0.8\n",
    "alpha_validation=0.9\n",
    "alpha_testing=0.4\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(x = training_set.T[0], y = training_set.T[1], label = \"Train (\"+ str(len(training_set))+\")\", alpha = alpha_trainig,color=color_trainig,s=size_trainig,marker=marker_trainig)\n",
    "\n",
    "ax.scatter(x = validation_set.T[0], y = validation_set.T[1], label = \"Valid. (\" + str(len(validation_set))+\")\", alpha = alpha_validation,color=color_validation,s=size_validation,marker=marker_validation)\n",
    "\n",
    "ax.scatter(x = test_set.T[0], y = test_set.T[1], label = \"Test (\" + str(len(test_set))+\")\", alpha = alpha_testing,color=color_testing,s=size_testing,marker=marker_testing)\n",
    "\n",
    "\n",
    "ax.scatter(x = stable_coordinates.T[0], y = stable_coordinates.T[1], label = \"Stable\", alpha = 0.8,color='black',s=11,marker=\"s\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('')\n",
    "\n",
    "plt.annotate('Neutrons', xy=(0.35, 0.1), xycoords='axes fraction',\n",
    "             ha='center', va='top', fontsize=45) \n",
    "plt.ylabel('')\n",
    "\n",
    "plt.annotate('Protons', xy=(0.05,0.7), xycoords='axes fraction',\n",
    "             ha='center', va='top', fontsize=45,rotation =90) \n",
    "\n",
    "\n",
    "# plt.ylabel(\"Protons\",fontsize=45)\n",
    "plt.legend(fontsize=34,markerscale=3 ,loc =(0.53,0.03))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the different models\n",
    "\n",
    "While we've defined the base for our perfect, good, intermediate, and bad models above, we now consider the situation where slight (statistical, in this case) variations of these models are introduced as new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# masses_truth = LDM_extended(truth_params,[input_NZ[\"Z\"], input_NZ[\"N\"]]) \n",
    "\n",
    "models_output = {}\n",
    "models_output_train = {}\n",
    "models_output_validation = {}\n",
    "models_output_test = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Option 1: Constructing the data by just adding a random noise to the output of the models\n",
    "# #Perfect Models Loop\n",
    "# params = PerfectM_params\n",
    "# for i in range(n_perfect):\n",
    "#     models_output[str(\"PerfectModel_\")+str(i)] = LDM_extended(params,[input_NZ[\"Z\"], input_NZ[\"N\"]])+ np.random.normal(0,1,size=len(input_NZ)) * noise_term_p\n",
    "\n",
    "\n",
    "# #Good Models Loop\n",
    "# params = GoodM_params\n",
    "# for i in range(n_good):\n",
    "#     models_output[str(\"GoodModel_\")+str(i)] = LDM_extended(params,[input_NZ[\"Z\"], input_NZ[\"N\"]])+ np.random.normal(0,1,size=len(input_NZ)) * noise_term_g\n",
    "\n",
    "# #Bad Models Loop\n",
    "# params = BadM_params\n",
    "# for i in range(n_bad):\n",
    "#     models_output[str(\"IntermediateModel_\")+str(i)] = LDM_extended(params,[input_NZ[\"Z\"], input_NZ[\"N\"]])+ np.random.normal(0,1,size=len(input_NZ)) * noise_term_b\n",
    "\n",
    "# #Terrible Models Loop\n",
    "# params = TerribleM_params\n",
    "# for i in range(n_terrible):\n",
    "#     models_output[str(\"BadModel_\")+str(i)] = LDM_extended(params,[input_NZ[\"Z\"], input_NZ[\"N\"]])+ np.random.normal(0,1,size=len(input_NZ)) * noise_term_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Option 2: Constructing the data by adding a random noise to the parameters of the models, plus random noise to the output\n",
    "#Perfect Models Loop\n",
    "np.random.seed(142857)\n",
    "params = PerfectM_params\n",
    "for i in range(n_perfect):\n",
    "    ran_params=params + np.random.normal(0,1,size=len(params))*params*params_noise_term_p\n",
    "    models_output[str(\"PerfectModel_\")+str(i)] = LDM_extended(ran_params,Full_set.T)+np.random.normal(0,1,size=len(Full_set)) * overall_output_noise\n",
    "\n",
    "\n",
    "\n",
    "#Good Models Loop\n",
    "np.random.seed(542857)\n",
    "params = GoodM_params\n",
    "for i in range(n_good):\n",
    "\n",
    "    ran_params=params+ np.random.normal(0,1,size=len(params))*params*params_noise_term_g\n",
    "\n",
    "\n",
    "    models_output[str(\"GoodModel_\")+str(i)] = LDM_extended(ran_params\n",
    "                                                           ,Full_set.T)+ np.random.normal(0,1,size=len(Full_set)) * overall_output_noise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Bad Models Loop\n",
    "np.random.seed(342857)\n",
    "params = BadM_params\n",
    "for i in range(n_bad):\n",
    "    ran_params=params+ np.random.normal(0,1,size=len(params))*params*params_noise_term_b\n",
    "    models_output[str(\"IntermediateModel_\")+str(i)] = LDM_extended(ran_params,Full_set.T)+ np.random.normal(0,1,size=len(Full_set)) * overall_output_noise\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Terrible Models Loop\n",
    "np.random.seed(442857)\n",
    "params = TerribleM_params\n",
    "for i in range(n_terrible):\n",
    "    ran_params=params+ np.random.normal(0,1,size=len(params))*params*params_noise_term_t\n",
    "\n",
    "\n",
    "    models_output[str(\"BadModel_\")+str(i)] = LDM_extended(ran_params\n",
    "                                                          ,Full_set.T)+ np.random.normal(0,1,size=len(Full_set)) * overall_output_noise\n",
    "    \n",
    "\n",
    "\n",
    "#This is done here to have a list of pure models, without the ground truth\n",
    "key_list=list(models_output.keys())\n",
    "np.random.seed(7*142857)\n",
    "models_output['truth']=LDM_extended(truth_params,Full_set.T) + np.random.normal(0,1,size=len(Full_set))*overall_data_noise\n",
    "\n",
    "\n",
    "models_output = pd.DataFrame(models_output)\n",
    "models_output[\"N\"] = Full_set.T[0]\n",
    "models_output[\"Z\"] = Full_set.T[1]\n",
    "models_output[\"A\"] = models_output[\"N\"] + models_output[\"Z\"]\n",
    "\n",
    "\n",
    "\n",
    "#Separating outputs for the three regions\n",
    "models_output_train = models_output.iloc[train_coordinates]\n",
    "\n",
    "models_output_validation = models_output.iloc[validation_coordinates]\n",
    "\n",
    "models_output_test = models_output.iloc[test_coordinates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot those models to see how bad they truly are when compared to 'truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"xtick\", labelsize=25)\n",
    "plt.rc(\"ytick\", labelsize=25)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10), dpi=100)\n",
    "\n",
    "\n",
    "model_index=0\n",
    "class_index=0\n",
    "for n_vals in n_classes:\n",
    "    legend_flag=0\n",
    "    for i in range(n_vals):\n",
    "        if legend_flag==0:\n",
    "            legend_flag=1\n",
    "            ax.scatter(x = models_output[\"A\"], y = models_output[key_list[model_index]]/models_output[\"A\"], label = n_Labels[class_index], alpha = 0.4,color=colors[class_index],marker=markers[0],s=60)\n",
    "        else:\n",
    "            ax.scatter(x = models_output[\"A\"], y = models_output[key_list[model_index]]/models_output[\"A\"],  alpha = 0.3,color=colors[class_index],marker=markers[0],s=50)\n",
    "        model_index=model_index+1\n",
    "    class_index=class_index+1\n",
    "    legend_flag=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(x = models_output[\"A\"], y = models_output['truth']/models_output[\"A\"], label = \"Truth\", alpha = 0.7,color='k',s=20)\n",
    "\n",
    "\n",
    "plt.xlabel(\"A\",fontsize=35)\n",
    "plt.ylabel(\"BE/A MeV\",fontsize=35)\n",
    "plt.legend(fontsize=25,markerscale=2 )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"xtick\", labelsize=25)\n",
    "plt.rc(\"ytick\", labelsize=25)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10), dpi=100)\n",
    "\n",
    "\n",
    "model_index=0\n",
    "class_index=0\n",
    "for n_vals in n_classes:\n",
    "    legend_flag=0\n",
    "    for i in range(n_vals):\n",
    "        if legend_flag==0:\n",
    "            legend_flag=1\n",
    "            ax.scatter(x = models_output[\"A\"], y = models_output[key_list[model_index]], label = n_Labels[class_index], alpha = 0.4,color=colors[class_index],marker=markers[0],s=60)\n",
    "        else:\n",
    "            ax.scatter(x = models_output[\"A\"], y = models_output[key_list[model_index]],  alpha = 0.3,color=colors[class_index],marker=markers[0],s=50)\n",
    "        model_index=model_index+1\n",
    "    class_index=class_index+1\n",
    "    legend_flag=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(x = models_output[\"A\"], y = models_output['truth'], label = \"Truth\", alpha = 0.7,color='k',s=20)\n",
    "\n",
    "\n",
    "plt.xlabel(\"A\",fontsize=35)\n",
    "plt.ylabel(\"BE MeV\",fontsize=35)\n",
    "plt.legend(fontsize=25,markerscale=2 )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
